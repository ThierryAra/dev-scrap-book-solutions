{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chapter 3 of \"Web Scraping with Python\" you were able to learn more about the following subjects:\n",
    "* Tracking internal links of website\n",
    "  * According to pattern\n",
    "  * All internal links\n",
    "* Website mapping\n",
    "* Collecting data\n",
    "* Tracking through internet\n",
    "\n",
    "The following cells aim to practice the contents listed above. For any sugestions, contact *gabriel.vasconcelos@usp.br*\n",
    "\n",
    "Use the website https://scraping-cap3.netlify.app/ to answer this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup and other libraries you find useful\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the website https://scraping-cap3.netlify.app/ and pass it to a BeautifulSoup object \n",
    "# with proper error handling\n",
    "\n",
    "def getBS(site):\n",
    "    try:\n",
    "        html = urlopen(site)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    except URLError as e:\n",
    "        return None\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInternalURL(link: str):\n",
    "    url = 'https://scraping-cap3.netlify.app/'\n",
    "    if link.startswith('./'):\n",
    "        url += link[2:]\n",
    "    elif link.startswith('../'):\n",
    "        url += link[3:]\n",
    "    elif link.startswith('/'):\n",
    "        url += link[1:]\n",
    "    else:\n",
    "        url = link\n",
    "    \n",
    "    return url\n",
    "\n",
    "def getInternalLinks(site):\n",
    "    site = makeInternalURL(site)\n",
    "    \n",
    "    bs = getBS(site)\n",
    "    urls = [a['href'] for a in bs.find_all('a', {'href':re.compile('^(\\.)*\\/.+')})]\n",
    "    return set(urls)\n",
    "\n",
    "def getExternalLinks(site):\n",
    "    site = makeInternalURL(site)\n",
    "    print(site)\n",
    "    bs = getBS(site)\n",
    "    print(bs)\n",
    "    all_urls = set([a['href'] for a in bs.find_all('a', {'href':re.compile('.+')})])\n",
    "    in_urls  = getInternalLinks(site)\n",
    "\n",
    "    all_urls.difference_update(in_urls)\n",
    "    return (in_urls, all_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "Get all internal links from the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below\n",
    "# Tip: use BFS algorithm to do a full mapping of the domain\n",
    "\n",
    "def getAllInternalLinks():\n",
    "    sites = ['https://scraping-cap3.netlify.app/index.html']\n",
    "    visited = set()\n",
    "\n",
    "    while(len(sites) > 0):\n",
    "        site = makeInternalURL(sites.pop(0))\n",
    "        visited.add(site)\n",
    "        bs = getBS(site)\n",
    "        urls = getInternalLinks(site)\n",
    "        \n",
    "        for url in urls:\n",
    "            url = makeInternalURL(url)\n",
    "            if(not url in visited and not url in sites):\n",
    "                sites.append(url)\n",
    "    \n",
    "    return visited\n",
    "    #print(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "internalLink =  getAllInternalLinks()\n",
    "internalLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "Get all external links from the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below\n",
    "def getAllExternalLinks():\n",
    "    sites = ['https://scraping-cap3.netlify.app/index.html']\n",
    "    visited = set()\n",
    "\n",
    "    while(len(sites) > 0):\n",
    "        site = makeInternalURL(sites.pop(0))\n",
    "        visited.add(site)\n",
    "        \n",
    "        ex_urls, in_urls = getExternalLinks(site)\n",
    "        for url in in_urls:\n",
    "            url = makeInternalURL(url)\n",
    "            if(not url in visited and not url in sites):\n",
    "                sites.append(url)\n",
    "    \n",
    "    return visited\n",
    "    #print(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scraping-cap3.netlify.app/index.html\n",
      "<!DOCTYPE html>\n",
      "\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "<title>Página Inicial</title>\n",
      "<link href=\"./styles/index.css\" rel=\"stylesheet\"/>\n",
      "<link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\" rel=\"stylesheet\"/>\n",
      "</head>\n",
      "<body>\n",
      "<header>\n",
      "<nav>\n",
      "<ul>\n",
      "<li><a href=\"./bandas.html\">Bandas</a></li>\n",
      "<li><a href=\"./vocalistas.html\">Vocalistas</a></li>\n",
      "<li><a href=\"./guitarristas.html\">Guitarristas</a></li>\n",
      "<li><a href=\"./bateristas.html\">Bateristas</a></li>\n",
      "<li><a href=\"./baixistas.html\">Baixistas</a></li>\n",
      "<li><a href=\"./tecladistas.html\">Tecladistas</a></li>\n",
      "</ul>\n",
      "</nav>\n",
      "<div>\n",
      "<a href=\"./index.html\">\n",
      "<img alt=\"Rock logo\" src=\"./assets/hand.png\"/>\n",
      "<span>Rock Encyclopedia</span>\n",
      "</a>\n",
      "</div>\n",
      "</header>\n",
      "<main>\n",
      "<h1>Rock Encyclopedia</h1>\n",
      "<p>Site que  busca juntar a informação sobre as melhoras bandas de rock!! Let's go, kids!</p>\n",
      "<img src=\"https://pbs.twimg.com/media/EYa5pE-WoAIjNta.jpg\">\n",
      "</img></main>\n",
      "<footer>\n",
      "<ul>\n",
      "<li>Feito com carinho por <a href=\"https://github.com/kibonusp\">Gabriel Freitas</a> ❤️</li>\n",
      "<li><a href=\"https://github.com/kibonusp\"><i class=\"fa fa-2x fa-github\"></i></a></li>\n",
      "<li><a href=\"https://www.linkedin.com/in/gabrielfreitas-xv/\"><i aria-hidden=\"true\" class=\"fa fa-2x fa-linkedin-square\"></i></a></li>\n",
      "</ul>\n",
      "</footer>\n",
      "</body>\n",
      "</html>\n",
      "https://www.linkedin.com/in/gabrielfreitas-xv/\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_links \u001b[39m=\u001b[39m getAllExternalLinks()\n\u001b[1;32m      2\u001b[0m external_links\n",
      "Cell \u001b[0;32mIn [22], line 10\u001b[0m, in \u001b[0;36mgetAllExternalLinks\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m site \u001b[39m=\u001b[39m makeInternalURL(sites\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m))\n\u001b[1;32m      8\u001b[0m visited\u001b[39m.\u001b[39madd(site)\n\u001b[0;32m---> 10\u001b[0m ex_urls, in_urls \u001b[39m=\u001b[39m getExternalLinks(site)\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m in_urls:\n\u001b[1;32m     12\u001b[0m     url \u001b[39m=\u001b[39m makeInternalURL(url)\n",
      "Cell \u001b[0;32mIn [24], line 26\u001b[0m, in \u001b[0;36mgetExternalLinks\u001b[0;34m(site)\u001b[0m\n\u001b[1;32m     24\u001b[0m bs \u001b[39m=\u001b[39m getBS(site)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(bs)\n\u001b[0;32m---> 26\u001b[0m all_urls \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([a[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m bs\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m:re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m.+\u001b[39m\u001b[39m'\u001b[39m)})])\n\u001b[1;32m     27\u001b[0m in_urls  \u001b[39m=\u001b[39m getInternalLinks(site)\n\u001b[1;32m     29\u001b[0m all_urls\u001b[39m.\u001b[39mdifference_update(in_urls)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "external_links = getAllExternalLinks()\n",
    "external_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "Get the title of each page in the website, its url and the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "Create an adjacency list of the domain (directed graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use networkx to generate a visualization of the graph\n",
    "'''\n",
    "Create an adjacency list in the following format:\n",
    "\n",
    "adjacencyList = {\n",
    "    'node A': ['nodeB', 'nodeC'],\n",
    "    'node B': ['node C']\n",
    "}\n",
    "'''\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for fromSite in adjacencyList:\n",
    "    for toSite in adjacencyList[fromSite]:\n",
    "        G.add_edge(fromSite, toSite)\n",
    "        \n",
    "nodes = list(G.nodes)\n",
    "for node in nodes:\n",
    "    if len(node) == 1:\n",
    "        print(node)\n",
    "        G.remove_node(node)\n",
    "        \n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G, pos=nx.shell_layout(G), node_size=100, width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "From the website external links, choose one randomly and create a internet crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
